{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PySpark Data Sources","text":"<p>Custom Spark data sources for reading and writing data in Apache Spark, using the Python Data Source API.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install pyspark-data-sources\n</code></pre> <p>If you want to install all extra dependencies, use:</p> <pre><code>pip install pyspark-data-sources[all]\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<pre><code>from pyspark_datasources.fake import FakeDataSource\n\n# Register the data source\nspark.dataSource.register(FakeDataSource)\n\nspark.read.format(\"fake\").load().show()\n\n# For streaming data generation\nspark.readStream.format(\"fake\").load().writeStream.format(\"console\").start()\n</code></pre>"},{"location":"#data-sources","title":"Data Sources","text":"Data Source Short Name Description Dependencies GithubDataSource <code>github</code> Read pull requests from a Github repository None FakeDataSource <code>fake</code> Generate fake data using the <code>Faker</code> library <code>faker</code> HuggingFaceDatasets <code>huggingface</code> Read datasets from the HuggingFace Hub <code>datasets</code> StockDataSource <code>stock</code> Read stock data from Alpha Vantage None SimpleJsonDataSource <code>simplejson</code> Write JSON data to Databricks DBFS <code>databricks-sdk</code> GoogleSheetsDataSource <code>googlesheets</code> Read table from public Google Sheets document None KaggleDataSource <code>kaggle</code> Read datasets from Kaggle <code>kagglehub</code>, <code>pandas</code>"},{"location":"datasources/fake/","title":"FakeDataSource","text":"<p>Requires the <code>Faker</code> library. You can install it manually: <code>pip install faker</code> or use <code>pip install pyspark-data-sources[faker]</code>.</p> <p>               Bases: <code>DataSource</code></p> <p>A fake data source for PySpark to generate synthetic data using the <code>faker</code> library.</p> <p>This data source allows specifying a schema with field names that correspond to <code>faker</code> providers to generate random data for testing and development purposes.</p> <p>The default schema is <code>name string, date string, zipcode string, state string</code>, and the default number of rows is <code>3</code>. Both can be customized by users.</p> <p>Name: <code>fake</code></p> Notes <ul> <li>The fake data source relies on the <code>faker</code> library. Make sure it is installed and accessible.</li> <li>Only string type fields are supported, and each field name must correspond to a method name in   the <code>faker</code> library.</li> <li>When using the stream reader, <code>numRows</code> is the number of rows per microbatch.</li> </ul> <p>Examples:</p> <p>Register the data source.</p> <pre><code>&gt;&gt;&gt; from pyspark_datasources import FakeDataSource\n&gt;&gt;&gt; spark.dataSource.register(FakeDataSource)\n</code></pre> <p>Use the fake datasource with the default schema and default number of rows:</p> <pre><code>&gt;&gt;&gt; spark.read.format(\"fake\").load().show()\n+-----------+----------+-------+-------+\n|       name|      date|zipcode|  state|\n+-----------+----------+-------+-------+\n|Carlos Cobb|2018-07-15|  73003|Indiana|\n| Eric Scott|1991-08-22|  10085|  Idaho|\n| Amy Martin|1988-10-28|  68076| Oregon|\n+-----------+----------+-------+-------+\n</code></pre> <p>Use the fake datasource with a custom schema:</p> <pre><code>&gt;&gt;&gt; spark.read.format(\"fake\").schema(\"name string, company string\").load().show()\n+---------------------+--------------+\n|name                 |company       |\n+---------------------+--------------+\n|Tanner Brennan       |Adams Group   |\n|Leslie Maxwell       |Santiago Group|\n|Mrs. Jacqueline Brown|Maynard Inc   |\n+---------------------+--------------+\n</code></pre> <p>Use the fake datasource with a different number of rows:</p> <pre><code>&gt;&gt;&gt; spark.read.format(\"fake\").option(\"numRows\", 5).load().show()\n+--------------+----------+-------+------------+\n|          name|      date|zipcode|       state|\n+--------------+----------+-------+------------+\n|  Pam Mitchell|1988-10-20|  23788|   Tennessee|\n|Melissa Turner|1996-06-14|  30851|      Nevada|\n|  Brian Ramsey|2021-08-21|  55277|  Washington|\n|  Caitlin Reed|1983-06-22|  89813|Pennsylvania|\n| Douglas James|2007-01-18|  46226|     Alabama|\n+--------------+----------+-------+------------+\n</code></pre> <p>Streaming fake data:</p> <pre><code>&gt;&gt;&gt; stream = spark.readStream.format(\"fake\").load().writeStream.format(\"console\").start()\nBatch: 0\n+--------------+----------+-------+------------+\n|          name|      date|zipcode|       state|\n+--------------+----------+-------+------------+\n|    Tommy Diaz|1976-11-17|  27627|South Dakota|\n|Jonathan Perez|1986-02-23|  81307|Rhode Island|\n|  Julia Farmer|1990-10-10|  40482|    Virginia|\n+--------------+----------+-------+------------+\nBatch: 1\n...\n&gt;&gt;&gt; stream.stop()\n</code></pre> Source code in <code>pyspark_datasources/fake.py</code> <pre><code>class FakeDataSource(DataSource):\n    \"\"\"\n    A fake data source for PySpark to generate synthetic data using the `faker` library.\n\n    This data source allows specifying a schema with field names that correspond to `faker`\n    providers to generate random data for testing and development purposes.\n\n    The default schema is `name string, date string, zipcode string, state string`, and the\n    default number of rows is `3`. Both can be customized by users.\n\n    Name: `fake`\n\n    Notes\n    -----\n    - The fake data source relies on the `faker` library. Make sure it is installed and accessible.\n    - Only string type fields are supported, and each field name must correspond to a method name in\n      the `faker` library.\n    - When using the stream reader, `numRows` is the number of rows per microbatch.\n\n    Examples\n    --------\n    Register the data source.\n\n    &gt;&gt;&gt; from pyspark_datasources import FakeDataSource\n    &gt;&gt;&gt; spark.dataSource.register(FakeDataSource)\n\n    Use the fake datasource with the default schema and default number of rows:\n\n    &gt;&gt;&gt; spark.read.format(\"fake\").load().show()\n    +-----------+----------+-------+-------+\n    |       name|      date|zipcode|  state|\n    +-----------+----------+-------+-------+\n    |Carlos Cobb|2018-07-15|  73003|Indiana|\n    | Eric Scott|1991-08-22|  10085|  Idaho|\n    | Amy Martin|1988-10-28|  68076| Oregon|\n    +-----------+----------+-------+-------+\n\n    Use the fake datasource with a custom schema:\n\n    &gt;&gt;&gt; spark.read.format(\"fake\").schema(\"name string, company string\").load().show()\n    +---------------------+--------------+\n    |name                 |company       |\n    +---------------------+--------------+\n    |Tanner Brennan       |Adams Group   |\n    |Leslie Maxwell       |Santiago Group|\n    |Mrs. Jacqueline Brown|Maynard Inc   |\n    +---------------------+--------------+\n\n    Use the fake datasource with a different number of rows:\n\n    &gt;&gt;&gt; spark.read.format(\"fake\").option(\"numRows\", 5).load().show()\n    +--------------+----------+-------+------------+\n    |          name|      date|zipcode|       state|\n    +--------------+----------+-------+------------+\n    |  Pam Mitchell|1988-10-20|  23788|   Tennessee|\n    |Melissa Turner|1996-06-14|  30851|      Nevada|\n    |  Brian Ramsey|2021-08-21|  55277|  Washington|\n    |  Caitlin Reed|1983-06-22|  89813|Pennsylvania|\n    | Douglas James|2007-01-18|  46226|     Alabama|\n    +--------------+----------+-------+------------+\n\n    Streaming fake data:\n\n    &gt;&gt;&gt; stream = spark.readStream.format(\"fake\").load().writeStream.format(\"console\").start()\n    Batch: 0\n    +--------------+----------+-------+------------+\n    |          name|      date|zipcode|       state|\n    +--------------+----------+-------+------------+\n    |    Tommy Diaz|1976-11-17|  27627|South Dakota|\n    |Jonathan Perez|1986-02-23|  81307|Rhode Island|\n    |  Julia Farmer|1990-10-10|  40482|    Virginia|\n    +--------------+----------+-------+------------+\n    Batch: 1\n    ...\n    &gt;&gt;&gt; stream.stop()\n    \"\"\"\n\n    @classmethod\n    def name(cls):\n        return \"fake\"\n\n    def schema(self):\n        return \"name string, date string, zipcode string, state string\"\n\n    def reader(self, schema: StructType) -&gt; \"FakeDataSourceReader\":\n        _validate_faker_schema(schema)\n        return FakeDataSourceReader(schema, self.options)\n\n    def streamReader(self, schema) -&gt; \"FakeDataSourceStreamReader\":\n        _validate_faker_schema(schema)\n        return FakeDataSourceStreamReader(schema, self.options)\n</code></pre>"},{"location":"datasources/github/","title":"GithubDataSource","text":"<p>               Bases: <code>DataSource</code></p> <p>A DataSource for reading pull requests data from Github.</p> <p>Name: <code>github</code></p> <p>Schema: <code>id int, title string, author string, created_at string, updated_at string</code></p> <p>Examples:</p> <p>Register the data source.</p> <pre><code>&gt;&gt;&gt; from pyspark_datasources import GithubDataSource\n&gt;&gt;&gt; spark.dataSource.register(GithubDataSource)\n</code></pre> <p>Load pull requests data from a public Github repository.</p> <pre><code>&gt;&gt;&gt; spark.read.format(\"github\").load(\"apache/spark\").show()\n+---+--------------------+--------+--------------------+--------------------+\n| id|               title|  author|          created_at|          updated_at|\n+---+--------------------+--------+--------------------+--------------------+\n|  1|Initial commit      |  matei |2014-02-03T18:47:...|2014-02-03T18:47:...|\n|...|                 ...|     ...|                 ...|                 ...|\n+---+--------------------+--------+--------------------+--------------------+\n</code></pre> <p>Load pull requests data from a private Github repository.</p> <pre><code>&gt;&gt;&gt; spark.read.format(\"github\").option(\"token\", \"your-token\").load(\"owner/repo\").show()\n</code></pre> Source code in <code>pyspark_datasources/github.py</code> <pre><code>class GithubDataSource(DataSource):\n    \"\"\"\n    A DataSource for reading pull requests data from Github.\n\n    Name: `github`\n\n    Schema: `id int, title string, author string, created_at string, updated_at string`\n\n    Examples\n    --------\n    Register the data source.\n\n    &gt;&gt;&gt; from pyspark_datasources import GithubDataSource\n    &gt;&gt;&gt; spark.dataSource.register(GithubDataSource)\n\n    Load pull requests data from a public Github repository.\n\n    &gt;&gt;&gt; spark.read.format(\"github\").load(\"apache/spark\").show()\n    +---+--------------------+--------+--------------------+--------------------+\n    | id|               title|  author|          created_at|          updated_at|\n    +---+--------------------+--------+--------------------+--------------------+\n    |  1|Initial commit      |  matei |2014-02-03T18:47:...|2014-02-03T18:47:...|\n    |...|                 ...|     ...|                 ...|                 ...|\n    +---+--------------------+--------+--------------------+--------------------+\n\n    Load pull requests data from a private Github repository.\n\n    &gt;&gt;&gt; spark.read.format(\"github\").option(\"token\", \"your-token\").load(\"owner/repo\").show()\n    \"\"\"\n\n    @classmethod\n    def name(self):\n        return \"github\"\n\n    def schema(self):\n        return \"id int, title string, author string, created_at string, updated_at string\"\n\n    def reader(self, schema):\n        return GithubPullRequestReader(self.options)\n</code></pre>"},{"location":"datasources/googlesheets/","title":"GoogleSheetsDataSource","text":"<p>               Bases: <code>DataSource</code></p> <p>A DataSource for reading table from public Google Sheets.</p> <p>Name: <code>googlesheets</code></p> <p>Schema: By default, all columns are treated as strings and the header row defines the column names.</p> Options <ul> <li><code>url</code>: The URL of the Google Sheets document.</li> <li><code>path</code>: The ID of the Google Sheets document.</li> <li><code>sheet_id</code>: The ID of the worksheet within the document.</li> <li><code>has_header</code>: Whether the sheet has a header row. Default is <code>true</code>.</li> </ul> <p>Either <code>url</code> or <code>path</code> must be specified, but not both.</p> <p>Examples:</p> <p>Register the data source.</p> <pre><code>&gt;&gt;&gt; from pyspark_datasources import GoogleSheetsDataSource\n&gt;&gt;&gt; spark.dataSource.register(GoogleSheetsDataSource)\n</code></pre> <p>Load data from a public Google Sheets document using <code>path</code> and optional <code>sheet_id</code>.</p> <pre><code>&gt;&gt;&gt; spreadsheet_id = \"10pD8oRN3RTBJq976RKPWHuxYy0Qa_JOoGFpsaS0Lop0\"\n&gt;&gt;&gt; spark.read.format(\"googlesheets\").options(sheet_id=\"0\").load(spreadsheet_id).show()\n+-------+---------+---------+-------+\n|country| latitude|longitude|   name|\n+-------+---------+---------+-------+\n|     AD|42.546245| 1.601554|Andorra|\n|    ...|      ...|      ...|    ...|\n+-------+---------+---------+-------+\n</code></pre> <p>Load data from a public Google Sheets document using <code>url</code>.</p> <pre><code>&gt;&gt;&gt; url = \"https://docs.google.com/spreadsheets/d/10pD8oRN3RTBJq976RKPWHuxYy0Qa_JOoGFpsaS0Lop0/edit?gid=0#gid=0\"\n&gt;&gt;&gt; spark.read.format(\"googlesheets\").options(url=url).load().show()\n+-------+---------+--------+-------+\n|country| latitude|ongitude|   name|\n+-------+---------+--------+-------+\n|     AD|42.546245|1.601554|Andorra|\n|    ...|      ...|     ...|    ...|\n+-------+---------+--------+-------+\n</code></pre> <p>Specify custom schema.</p> <pre><code>&gt;&gt;&gt; schema = \"id string, lat double, long double, name string\"\n&gt;&gt;&gt; spark.read.format(\"googlesheets\").schema(schema).options(url=url).load().show()\n+---+---------+--------+-------+\n| id|      lat|    long|   name|\n+---+---------+--------+-------+\n| AD|42.546245|1.601554|Andorra|\n|...|      ...|     ...|    ...|\n+---+---------+--------+-------+\n</code></pre> <p>Treat first row as data instead of header.</p> <pre><code>&gt;&gt;&gt; schema = \"c1 string, c2 string, c3 string, c4 string\"\n&gt;&gt;&gt; spark.read.format(\"googlesheets\").schema(schema).options(url=url, has_header=\"false\").load().show()\n+-------+---------+---------+-------+\n|     c1|       c2|       c3|     c4|\n+-------+---------+---------+-------+\n|country| latitude|longitude|   name|\n|     AD|42.546245| 1.601554|Andorra|\n|    ...|      ...|      ...|    ...|\n+-------+---------+---------+-------+\n</code></pre> Source code in <code>pyspark_datasources/googlesheets.py</code> <pre><code>class GoogleSheetsDataSource(DataSource):\n    \"\"\"\n    A DataSource for reading table from public Google Sheets.\n\n    Name: `googlesheets`\n\n    Schema: By default, all columns are treated as strings and the header row defines the column names.\n\n    Options\n    --------\n    - `url`: The URL of the Google Sheets document.\n    - `path`: The ID of the Google Sheets document.\n    - `sheet_id`: The ID of the worksheet within the document.\n    - `has_header`: Whether the sheet has a header row. Default is `true`.\n\n    Either `url` or `path` must be specified, but not both.\n\n    Examples\n    --------\n    Register the data source.\n\n    &gt;&gt;&gt; from pyspark_datasources import GoogleSheetsDataSource\n    &gt;&gt;&gt; spark.dataSource.register(GoogleSheetsDataSource)\n\n    Load data from a public Google Sheets document using `path` and optional `sheet_id`.\n\n    &gt;&gt;&gt; spreadsheet_id = \"10pD8oRN3RTBJq976RKPWHuxYy0Qa_JOoGFpsaS0Lop0\"\n    &gt;&gt;&gt; spark.read.format(\"googlesheets\").options(sheet_id=\"0\").load(spreadsheet_id).show()\n    +-------+---------+---------+-------+\n    |country| latitude|longitude|   name|\n    +-------+---------+---------+-------+\n    |     AD|42.546245| 1.601554|Andorra|\n    |    ...|      ...|      ...|    ...|\n    +-------+---------+---------+-------+\n\n    Load data from a public Google Sheets document using `url`.\n\n    &gt;&gt;&gt; url = \"https://docs.google.com/spreadsheets/d/10pD8oRN3RTBJq976RKPWHuxYy0Qa_JOoGFpsaS0Lop0/edit?gid=0#gid=0\"\n    &gt;&gt;&gt; spark.read.format(\"googlesheets\").options(url=url).load().show()\n    +-------+---------+--------+-------+\n    |country| latitude|ongitude|   name|\n    +-------+---------+--------+-------+\n    |     AD|42.546245|1.601554|Andorra|\n    |    ...|      ...|     ...|    ...|\n    +-------+---------+--------+-------+\n\n    Specify custom schema.\n\n    &gt;&gt;&gt; schema = \"id string, lat double, long double, name string\"\n    &gt;&gt;&gt; spark.read.format(\"googlesheets\").schema(schema).options(url=url).load().show()\n    +---+---------+--------+-------+\n    | id|      lat|    long|   name|\n    +---+---------+--------+-------+\n    | AD|42.546245|1.601554|Andorra|\n    |...|      ...|     ...|    ...|\n    +---+---------+--------+-------+\n\n    Treat first row as data instead of header.\n\n    &gt;&gt;&gt; schema = \"c1 string, c2 string, c3 string, c4 string\"\n    &gt;&gt;&gt; spark.read.format(\"googlesheets\").schema(schema).options(url=url, has_header=\"false\").load().show()\n    +-------+---------+---------+-------+\n    |     c1|       c2|       c3|     c4|\n    +-------+---------+---------+-------+\n    |country| latitude|longitude|   name|\n    |     AD|42.546245| 1.601554|Andorra|\n    |    ...|      ...|      ...|    ...|\n    +-------+---------+---------+-------+\n    \"\"\"\n\n    @classmethod\n    def name(self):\n        return \"googlesheets\"\n\n    def __init__(self, options: Dict[str, str]):\n        if \"url\" in options:\n            sheet = Sheet.from_url(options.pop(\"url\"))\n        elif \"path\" in options:\n            sheet = Sheet(options.pop(\"path\"), options.pop(\"sheet_id\", None))\n        else:\n            raise ValueError(\n                \"You must specify either `url` or `path` (spreadsheet ID).\"\n            )\n        has_header = options.pop(\"has_header\", \"true\").lower() == \"true\"\n        self.parameters = Parameters(sheet, has_header)\n\n    def schema(self) -&gt; StructType:\n        if not self.parameters.has_header:\n            raise ValueError(\"Custom schema is required when `has_header` is false\")\n\n        import pandas as pd\n\n        # Read schema from the first row of the sheet\n        df = pd.read_csv(self.parameters.sheet.get_query_url(\"select * limit 1\"))\n        return StructType([StructField(col, StringType()) for col in df.columns])\n\n    def reader(self, schema: StructType) -&gt; DataSourceReader:\n        return GoogleSheetsReader(self.parameters, schema)\n</code></pre>"},{"location":"datasources/huggingface/","title":"HuggingFaceDatasets","text":"<p>Requires the <code>datasets</code> library.</p> <p>               Bases: <code>DataSource</code></p> <p>An example data source for reading HuggingFace Datasets in Spark.</p> <p>This data source allows reading public datasets from the HuggingFace Hub directly into Spark DataFrames. The schema is automatically inferred from the dataset features. The split can be specified using the <code>split</code> option. The default split is <code>train</code>.</p> <p>Name: <code>huggingface</code></p> Notes: <ul> <li>Please use the official HuggingFace Datasets API: https://github.com/huggingface/pyspark_huggingface.</li> <li>The HuggingFace <code>datasets</code> library is required to use this data source. Make sure it is installed.</li> <li>If the schema is automatically inferred, it will use string type for all fields.</li> <li>Currently it can only be used with public datasets. Private or gated ones are not supported.</li> </ul> <p>Examples:</p> <p>Register the data source.</p> <pre><code>&gt;&gt;&gt; from pyspark_datasources import HuggingFaceDatasets\n&gt;&gt;&gt; spark.dataSource.register(HuggingFaceDatasets)\n</code></pre> <p>Load a public dataset from the HuggingFace Hub.</p> <pre><code>&gt;&gt;&gt; spark.read.format(\"huggingface\").load(\"imdb\").show()\n+--------------------+-----+\n|                text|label|\n+--------------------+-----+\n|I rented I AM CUR...|    0|\n|\"I Am Curious: Ye...|    0|\n|...                 |  ...|\n+--------------------+-----+\n</code></pre> <p>Load a specific split from a public dataset from the HuggingFace Hub.</p> <pre><code>&gt;&gt;&gt; spark.read.format(\"huggingface\").option(\"split\", \"test\").load(\"imdb\").show()\n+--------------------+-----+\n|                text|label|\n+--------------------+-----+\n|I love sci-fi and...|    0|\n|Worth the enterta...|    0|\n|...                 |  ...|\n+--------------------+-----+\n</code></pre> Source code in <code>pyspark_datasources/huggingface.py</code> <pre><code>class HuggingFaceDatasets(DataSource):\n    \"\"\"\n    An example data source for reading HuggingFace Datasets in Spark.\n\n    This data source allows reading public datasets from the HuggingFace Hub directly into Spark\n    DataFrames. The schema is automatically inferred from the dataset features. The split can be\n    specified using the `split` option. The default split is `train`.\n\n    Name: `huggingface`\n\n    Notes:\n    -----\n    - Please use the official HuggingFace Datasets API: https://github.com/huggingface/pyspark_huggingface.\n    - The HuggingFace `datasets` library is required to use this data source. Make sure it is installed.\n    - If the schema is automatically inferred, it will use string type for all fields.\n    - Currently it can only be used with public datasets. Private or gated ones are not supported.\n\n    Examples\n    --------\n    Register the data source.\n\n    &gt;&gt;&gt; from pyspark_datasources import HuggingFaceDatasets\n    &gt;&gt;&gt; spark.dataSource.register(HuggingFaceDatasets)\n\n    Load a public dataset from the HuggingFace Hub.\n\n    &gt;&gt;&gt; spark.read.format(\"huggingface\").load(\"imdb\").show()\n    +--------------------+-----+\n    |                text|label|\n    +--------------------+-----+\n    |I rented I AM CUR...|    0|\n    |\"I Am Curious: Ye...|    0|\n    |...                 |  ...|\n    +--------------------+-----+\n\n    Load a specific split from a public dataset from the HuggingFace Hub.\n\n    &gt;&gt;&gt; spark.read.format(\"huggingface\").option(\"split\", \"test\").load(\"imdb\").show()\n    +--------------------+-----+\n    |                text|label|\n    +--------------------+-----+\n    |I love sci-fi and...|    0|\n    |Worth the enterta...|    0|\n    |...                 |  ...|\n    +--------------------+-----+\n    \"\"\"\n\n    def __init__(self, options):\n        super().__init__(options)\n        if \"path\" not in options or not options[\"path\"]:\n            raise Exception(\"You must specify a dataset name in`.load()`.\")\n\n    @classmethod\n    def name(cls):\n        return \"huggingface\"\n\n    def schema(self):\n        # The imports must be inside the method to be serializable.\n        from datasets import load_dataset_builder\n        dataset_name = self.options[\"path\"]\n        ds_builder = load_dataset_builder(dataset_name)\n        features = ds_builder.info.features\n        if features is None:\n            raise Exception(\n                \"Unable to automatically determine the schema using the dataset features. \"\n                \"Please specify the schema manually using `.schema()`.\"\n            )\n        schema = StructType()\n        for key, value in features.items():\n            # For simplicity, use string for all values.\n            schema.add(StructField(key, StringType(), True))\n        return schema\n\n    def reader(self, schema: StructType) -&gt; \"DataSourceReader\":\n        return HuggingFaceDatasetsReader(schema, self.options)\n</code></pre>"},{"location":"datasources/kaggle/","title":"KaggleDataSource","text":"<p>Requires the <code>kagglehub</code> library.</p> <p>               Bases: <code>DataSource</code></p> <p>A DataSource for reading Kaggle datasets in Spark.</p> <p>This data source allows reading datasets from Kaggle directly into Spark DataFrames.</p> <p>Name: <code>kaggle</code></p> Options <ul> <li><code>handle</code>: The dataset handle on Kaggle, in the form of <code>{owner_slug}/{dataset_slug}</code>     or <code>{owner_slug}/{dataset_slug}/versions/{version_number}</code></li> <li><code>path</code>: The path to a file within the dataset.</li> <li><code>username</code>: The Kaggle username for authentication.</li> <li><code>key</code>: The Kaggle API key for authentication.</li> </ul> Notes: <ul> <li>The <code>kagglehub</code> library is required to use this data source. Make sure it is installed.</li> <li>To read private datasets or datasets that require user authentication, <code>username</code> and <code>key</code> must be provided.</li> <li>Currently all data is read from a single partition.</li> </ul> <p>Examples:</p> <p>Register the data source.</p> <pre><code>&gt;&gt;&gt; from pyspark_datasources import KaggleDataSource\n&gt;&gt;&gt; spark.dataSource.register(KaggleDataSource)\n</code></pre> <p>Load a public dataset from Kaggle.</p> <pre><code>&gt;&gt;&gt; spark.read.format(\"kaggle\").options(handle=\"yasserh/titanic-dataset\").load(\"Titanic-Dataset.csv\").select(\"Name\").show()\n+--------------------+\n|                Name|\n+--------------------+\n|Braund, Mr. Owen ...|\n|Cumings, Mrs. Joh...|\n|...                 |\n+--------------------+\n</code></pre> <p>Load a private dataset with authentication.</p> <pre><code>&gt;&gt;&gt; spark.read.format(\"kaggle\").options(\n...     username=\"myaccount\",\n...     key=\"&lt;token&gt;\",\n...     handle=\"myaccount/my-private-dataset\",\n... ).load(\"file.csv\").show()\n</code></pre> Source code in <code>pyspark_datasources/kaggle.py</code> <pre><code>class KaggleDataSource(DataSource):\n    \"\"\"\n    A DataSource for reading Kaggle datasets in Spark.\n\n    This data source allows reading datasets from Kaggle directly into Spark DataFrames.\n\n    Name: `kaggle`\n\n    Options\n    -------\n    - `handle`: The dataset handle on Kaggle, in the form of `{owner_slug}/{dataset_slug}`\n        or `{owner_slug}/{dataset_slug}/versions/{version_number}`\n    - `path`: The path to a file within the dataset.\n    - `username`: The Kaggle username for authentication.\n    - `key`: The Kaggle API key for authentication.\n\n    Notes:\n    -----\n    - The `kagglehub` library is required to use this data source. Make sure it is installed.\n    - To read private datasets or datasets that require user authentication, `username` and `key` must be provided.\n    - Currently all data is read from a single partition.\n\n    Examples\n    --------\n    Register the data source.\n\n    &gt;&gt;&gt; from pyspark_datasources import KaggleDataSource\n    &gt;&gt;&gt; spark.dataSource.register(KaggleDataSource)\n\n    Load a public dataset from Kaggle.\n\n    &gt;&gt;&gt; spark.read.format(\"kaggle\").options(handle=\"yasserh/titanic-dataset\").load(\"Titanic-Dataset.csv\").select(\"Name\").show()\n    +--------------------+\n    |                Name|\n    +--------------------+\n    |Braund, Mr. Owen ...|\n    |Cumings, Mrs. Joh...|\n    |...                 |\n    +--------------------+\n\n    Load a private dataset with authentication.\n\n    &gt;&gt;&gt; spark.read.format(\"kaggle\").options(\n    ...     username=\"myaccount\",\n    ...     key=\"&lt;token&gt;\",\n    ...     handle=\"myaccount/my-private-dataset\",\n    ... ).load(\"file.csv\").show()\n    \"\"\"\n\n    @classmethod\n    def name(cls) -&gt; str:\n        return \"kaggle\"\n\n    @cached_property\n    def _data(self) -&gt; \"pa.Table\":\n        import ast\n        import os\n\n        import pyarrow as pa\n\n        handle = self.options.pop(\"handle\")\n        path = self.options.pop(\"path\")\n        username = self.options.pop(\"username\", None)\n        key = self.options.pop(\"key\", None)\n        if username or key:\n            if not (username and key):\n                raise ValueError(\n                    \"Both username and key must be provided to authenticate.\"\n                )\n            os.environ[\"KAGGLE_USERNAME\"] = username\n            os.environ[\"KAGGLE_KEY\"] = key\n\n        kwargs = {k: ast.literal_eval(v) for k, v in self.options.items()}\n\n        # Cache in a temporary directory to avoid writing to ~ which may be read-only\n        with tempfile.TemporaryDirectory() as tmpdir:\n            os.environ[\"KAGGLEHUB_CACHE\"] = tmpdir\n            import kagglehub\n\n            df = kagglehub.dataset_load(\n                kagglehub.KaggleDatasetAdapter.PANDAS,\n                handle,\n                path,\n                **kwargs,\n            )\n            return pa.Table.from_pandas(df)\n\n    def schema(self) -&gt; StructType:\n        return from_arrow_schema(self._data.schema)\n\n    def reader(self, schema: StructType) -&gt; \"KaggleDataReader\":\n        return KaggleDataReader(self)\n</code></pre>"},{"location":"datasources/simplejson/","title":"SimpleJsonDataSource","text":"<p>               Bases: <code>DataSource</code></p> <p>A simple json writer for writing data to Databricks DBFS.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pyspark.sql.functions as sf\n&gt;&gt;&gt; df = spark.range(0, 10, 1, 2).withColumn(\"value\", sf.expr(\"concat('value_', id)\"))\n</code></pre> <p>Register the data source.</p> <pre><code>&gt;&gt;&gt; from pyspark_datasources import SimpleJsonDataSource\n&gt;&gt;&gt; spark.dataSource.register(SimpleJsonDataSource)\n</code></pre> <p>Append the DataFrame to a DBFS path as json files.</p> <pre><code>&gt;&gt;&gt; (\n...     df.write.format(\"simplejson\")\n...     .mode(\"append\")\n...     .option(\"databricks_url\", \"https://your-databricks-instance.cloud.databricks.com\")\n...     .option(\"databricks_token\", \"your-token\")\n...     .save(\"/path/to/output\")\n... )\n</code></pre> <p>Overwrite the DataFrame to a DBFS path as json files.</p> <pre><code>&gt;&gt;&gt; (\n...     df.write.format(\"simplejson\")\n...     .mode(\"overwrite\")\n...     .option(\"databricks_url\", \"https://your-databricks-instance.cloud.databricks.com\")\n...     .option(\"databricks_token\", \"your-token\")\n...     .save(\"/path/to/output\")\n... )\n</code></pre> Source code in <code>pyspark_datasources/simplejson.py</code> <pre><code>class SimpleJsonDataSource(DataSource):\n    \"\"\"\n    A simple json writer for writing data to Databricks DBFS.\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; import pyspark.sql.functions as sf\n    &gt;&gt;&gt; df = spark.range(0, 10, 1, 2).withColumn(\"value\", sf.expr(\"concat('value_', id)\"))\n\n    Register the data source.\n\n    &gt;&gt;&gt; from pyspark_datasources import SimpleJsonDataSource\n    &gt;&gt;&gt; spark.dataSource.register(SimpleJsonDataSource)\n\n    Append the DataFrame to a DBFS path as json files.\n\n    &gt;&gt;&gt; (\n    ...     df.write.format(\"simplejson\")\n    ...     .mode(\"append\")\n    ...     .option(\"databricks_url\", \"https://your-databricks-instance.cloud.databricks.com\")\n    ...     .option(\"databricks_token\", \"your-token\")\n    ...     .save(\"/path/to/output\")\n    ... )\n\n    Overwrite the DataFrame to a DBFS path as json files.\n\n    &gt;&gt;&gt; (\n    ...     df.write.format(\"simplejson\")\n    ...     .mode(\"overwrite\")\n    ...     .option(\"databricks_url\", \"https://your-databricks-instance.cloud.databricks.com\")\n    ...     .option(\"databricks_token\", \"your-token\")\n    ...     .save(\"/path/to/output\")\n    ... )\n    \"\"\"\n    @classmethod\n    def name(self) -&gt; str:\n        return \"simplejson\"\n\n    def writer(self, schema: StructType, overwrite: bool):\n        return SimpleJsonWriter(schema, self.options, overwrite)\n</code></pre>"},{"location":"datasources/stock/","title":"StockDataSource","text":"<p>               Bases: <code>DataSource</code></p> <p>A data source for reading stock data using the Alpha Vantage API.</p> <p>Examples:</p> <p>Load the daily stock data for SPY:</p> <pre><code>&gt;&gt;&gt; df = spark.read.format(\"stock\").option(\"api_key\", \"your-key\").load(\"SPY\")\n&gt;&gt;&gt; df.show(n=5)\n+----------+------+------+------+------+--------+------+\n|      date|  open|  high|   low| close|  volume|symbol|\n+----------+------+------+------+------+--------+------+\n|2024-06-04|526.46|529.15|524.96|528.39|33898396|   SPY|\n|2024-06-03|529.02|529.31| 522.6| 527.8|46835702|   SPY|\n|2024-05-31|523.59| 527.5|518.36|527.37|90785755|   SPY|\n|2024-05-30|524.52| 525.2|521.33|522.61|46468510|   SPY|\n|2024-05-29|525.68|527.31|525.37| 526.1|45190323|   SPY|\n+----------+------+------+------+------+--------+------+\n</code></pre> Source code in <code>pyspark_datasources/stock.py</code> <pre><code>class StockDataSource(DataSource):\n    \"\"\"\n    A data source for reading stock data using the Alpha Vantage API.\n\n    Examples\n    --------\n\n    Load the daily stock data for SPY:\n\n    &gt;&gt;&gt; df = spark.read.format(\"stock\").option(\"api_key\", \"your-key\").load(\"SPY\")\n    &gt;&gt;&gt; df.show(n=5)\n    +----------+------+------+------+------+--------+------+\n    |      date|  open|  high|   low| close|  volume|symbol|\n    +----------+------+------+------+------+--------+------+\n    |2024-06-04|526.46|529.15|524.96|528.39|33898396|   SPY|\n    |2024-06-03|529.02|529.31| 522.6| 527.8|46835702|   SPY|\n    |2024-05-31|523.59| 527.5|518.36|527.37|90785755|   SPY|\n    |2024-05-30|524.52| 525.2|521.33|522.61|46468510|   SPY|\n    |2024-05-29|525.68|527.31|525.37| 526.1|45190323|   SPY|\n    +----------+------+------+------+------+--------+------+\n    \"\"\"\n    @classmethod\n    def name(self) -&gt; str:\n        return \"stock\"\n\n    def schema(self) -&gt; str:\n        return (\n            \"date string, open double, high double, \"\n            \"low double, close double, volume long, symbol string\"\n        )\n\n    def reader(self, schema):\n        return StockDataReader(schema, self.options)\n</code></pre>"}]}