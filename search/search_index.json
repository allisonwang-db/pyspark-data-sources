{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PySpark Data Sources","text":"<p>Custom Spark data sources for reading and writing data in Apache Spark, using the Python Data Source API.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install pyspark-data-sources\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<pre><code>from pyspark_datasources.github import GithubDataSource\n\n# Register the data source\nspark.dataSource.register(GithubDataSource)\n\nspark.read.format(\"github\").load(\"apache/spark\").show()\n</code></pre>"},{"location":"#data-sources","title":"Data Sources","text":"Data Source Short Name Description GithubDataSource <code>github</code> Read pull requests from a Github repository FakeDataSource <code>fake</code> Generate fake data using the <code>Faker</code> library"},{"location":"datasources/fake/","title":"FakeDataSource","text":"<p>Requires the <code>Faker</code> library. You can install it manually: <code>pip install faker</code> or use <code>pip install pyspark-data-sources[faker]</code>.</p> <p>             Bases: <code>DataSource</code></p> <p>A fake data source for PySpark to generate synthetic data using the <code>faker</code> library.</p> <p>This data source allows specifying a schema with field names that correspond to <code>faker</code> providers to generate random data for testing and development purposes.</p> <p>The default schema is <code>name string, date string, zipcode string, state string</code>, and the default number of rows is <code>3</code>. Both can be customized by users.</p> <p>Examples:</p> <p>Register the data source.</p> <pre><code>&gt;&gt;&gt; from pyspark_datasources.fake import FakeDataSource\n&gt;&gt;&gt; spark.dataSource.register(FakeDataSource)\n</code></pre> <p>Use the fake datasource with the default schema and default number of rows:</p> <pre><code>&gt;&gt;&gt; spark.read.format(\"fake\").load().show()\n+-----------+----------+-------+-------+\n|       name|      date|zipcode|  state|\n+-----------+----------+-------+-------+\n|Carlos Cobb|2018-07-15|  73003|Indiana|\n| Eric Scott|1991-08-22|  10085|  Idaho|\n| Amy Martin|1988-10-28|  68076| Oregon|\n+-----------+----------+-------+-------+\n</code></pre> <p>Use the fake datasource with a custom schema:</p> <pre><code>&gt;&gt;&gt; spark.read.format(\"fake\").schema(\"name string, company string\").load().show()\n+---------------------+--------------+\n|name                 |company       |\n+---------------------+--------------+\n|Tanner Brennan       |Adams Group   |\n|Leslie Maxwell       |Santiago Group|\n|Mrs. Jacqueline Brown|Maynard Inc   |\n+---------------------+--------------+\n</code></pre> <p>Use the fake datasource with a different number of rows:</p> <pre><code>&gt;&gt;&gt; spark.read.format(\"fake\").option(\"numRows\", 5).load().show()\n+--------------+----------+-------+------------+\n|          name|      date|zipcode|       state|\n+--------------+----------+-------+------------+\n|  Pam Mitchell|1988-10-20|  23788|   Tennessee|\n|Melissa Turner|1996-06-14|  30851|      Nevada|\n|  Brian Ramsey|2021-08-21|  55277|  Washington|\n|  Caitlin Reed|1983-06-22|  89813|Pennsylvania|\n| Douglas James|2007-01-18|  46226|     Alabama|\n+--------------+----------+-------+------------+\n</code></pre> Notes <p>The fake data source relies on the <code>faker</code> library. Make sure it is installed and accessible in your environment to use this data source. Only string type fields are supported, and each field name must correspond to a method name in the <code>faker</code> library.</p> Source code in <code>pyspark_datasources/fake.py</code> <pre><code>class FakeDataSource(DataSource):\n    \"\"\"\n    A fake data source for PySpark to generate synthetic data using the `faker` library.\n\n    This data source allows specifying a schema with field names that correspond to `faker`\n    providers to generate random data for testing and development purposes.\n\n    The default schema is `name string, date string, zipcode string, state string`, and the\n    default number of rows is `3`. Both can be customized by users.\n\n    Examples\n    --------\n    Register the data source.\n\n    &gt;&gt;&gt; from pyspark_datasources.fake import FakeDataSource\n    &gt;&gt;&gt; spark.dataSource.register(FakeDataSource)\n\n    Use the fake datasource with the default schema and default number of rows:\n\n    &gt;&gt;&gt; spark.read.format(\"fake\").load().show()\n    +-----------+----------+-------+-------+\n    |       name|      date|zipcode|  state|\n    +-----------+----------+-------+-------+\n    |Carlos Cobb|2018-07-15|  73003|Indiana|\n    | Eric Scott|1991-08-22|  10085|  Idaho|\n    | Amy Martin|1988-10-28|  68076| Oregon|\n    +-----------+----------+-------+-------+\n\n    Use the fake datasource with a custom schema:\n\n    &gt;&gt;&gt; spark.read.format(\"fake\").schema(\"name string, company string\").load().show()\n    +---------------------+--------------+\n    |name                 |company       |\n    +---------------------+--------------+\n    |Tanner Brennan       |Adams Group   |\n    |Leslie Maxwell       |Santiago Group|\n    |Mrs. Jacqueline Brown|Maynard Inc   |\n    +---------------------+--------------+\n\n    Use the fake datasource with a different number of rows:\n\n    &gt;&gt;&gt; spark.read.format(\"fake\").option(\"numRows\", 5).load().show()\n    +--------------+----------+-------+------------+\n    |          name|      date|zipcode|       state|\n    +--------------+----------+-------+------------+\n    |  Pam Mitchell|1988-10-20|  23788|   Tennessee|\n    |Melissa Turner|1996-06-14|  30851|      Nevada|\n    |  Brian Ramsey|2021-08-21|  55277|  Washington|\n    |  Caitlin Reed|1983-06-22|  89813|Pennsylvania|\n    | Douglas James|2007-01-18|  46226|     Alabama|\n    +--------------+----------+-------+------------+\n\n    Notes\n    -----\n    The fake data source relies on the `faker` library. Make sure it is installed and accessible\n    in your environment to use this data source. Only string type fields are supported, and each\n    field name must correspond to a method name in the `faker` library.\n    \"\"\"\n\n    @classmethod\n    def name(cls):\n        return \"fake\"\n\n    def schema(self):\n        return \"name string, date string, zipcode string, state string\"\n\n    def reader(self, schema: StructType):\n        # Verify the library is installed correctly.\n        try:\n            from faker import Faker\n        except ImportError:\n            raise Exception(\"You need to install `faker` to use the fake datasource.\")\n\n        # Check the schema is valid before proceed to reading.\n        fake = Faker()\n        for field in schema.fields:\n            try:\n                getattr(fake, field.name)()\n            except AttributeError:\n                raise Exception(\n                    f\"Unable to find a method called `{field.name}` in faker. \"\n                    f\"Please check Faker's documentation to see supported methods.\"\n                )\n            if field.dataType != StringType():\n                raise Exception(\n                    f\"Field `{field.name}` is not a StringType. \"\n                    f\"Only StringType is supported in the fake datasource.\"\n                )\n\n        return FakeDataSourceReader(schema, self.options)\n</code></pre>"},{"location":"datasources/github/","title":"GithubDataSource","text":"<p>             Bases: <code>DataSource</code></p> <p>A DataSource for reading pull requests data from Github.</p> <p>Name: <code>github</code></p> <p>Schema: <code>id int, title string, author string, created_at string, updated_at string</code></p> <p>Examples:</p> <p>Register the data source.</p> <pre><code>&gt;&gt;&gt; from pyspark_datasources.github import GithubDataSource\n&gt;&gt;&gt; spark.dataSource.register(GithubDataSource)\n</code></pre> <p>Load pull requests data from a public Github repository.</p> <pre><code>&gt;&gt;&gt; spark.read.format(\"github\").load(\"apache/spark\").show()\n+---+--------------------+--------+--------------------+--------------------+\n| id|               title|  author|          created_at|          updated_at|\n+---+--------------------+--------+--------------------+--------------------+\n|  1|Initial commit      |  matei |2014-02-03T18:47:...|2014-02-03T18:47:...|\n...\n</code></pre> <p>Load pull requests data from a private Github repository.</p> <pre><code>&gt;&gt;&gt; spark.read.format(\"github\").option(\"token\", \"your-token\").load(\"owner/repo\").show()\n</code></pre> Source code in <code>pyspark_datasources/github.py</code> <pre><code>class GithubDataSource(DataSource):\n    \"\"\"\n    A DataSource for reading pull requests data from Github.\n\n    Name: `github`\n\n    Schema: `id int, title string, author string, created_at string, updated_at string`\n\n    Examples\n    --------\n    Register the data source.\n\n    &gt;&gt;&gt; from pyspark_datasources.github import GithubDataSource\n    &gt;&gt;&gt; spark.dataSource.register(GithubDataSource)\n\n    Load pull requests data from a public Github repository.\n\n    &gt;&gt;&gt; spark.read.format(\"github\").load(\"apache/spark\").show()\n    +---+--------------------+--------+--------------------+--------------------+\n    | id|               title|  author|          created_at|          updated_at|\n    +---+--------------------+--------+--------------------+--------------------+\n    |  1|Initial commit      |  matei |2014-02-03T18:47:...|2014-02-03T18:47:...|\n    ...\n\n    Load pull requests data from a private Github repository.\n\n    &gt;&gt;&gt; spark.read.format(\"github\").option(\"token\", \"your-token\").load(\"owner/repo\").show()\n    \"\"\"\n\n    @classmethod\n    def name(self):\n        return \"github\"\n\n    def schema(self):\n        return \"id int, title string, author string, created_at string, updated_at string\"\n\n    def reader(self, schema):\n        return GithubPullRequestReader(self.options)\n</code></pre>"}]}