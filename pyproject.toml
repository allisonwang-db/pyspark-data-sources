[project]
name = "pyspark-data-sources"
version = "0.1.11"
description = "Custom Spark data sources for reading and writing data in Apache Spark, using the Python Data Source API"
readme = "README.md"
requires-python = ">=3.9,<3.13"
license = { file = "LICENSE" }
authors = [
  { name = "Allison Wang", email = "allison.wang@databricks.com" },
]
dependencies = [
  "pyarrow>=11.0.0",
  "requests>=2.31.0,<3.0.0",
  "faker>=23.1.0,<24.0.0",
  "mkdocstrings[python]>=0.28.0,<0.29.0",
]

[project.optional-dependencies]
faker = [
  "faker>=23.1.0,<24.0.0",
]
datasets = [
  "datasets>=2.17.0,<3.0.0",
]
databricks = [
  "databricks-sdk>=0.28.0,<0.29.0",
]
kaggle = [
  "kagglehub[pandas-datasets]>=0.3.10,<0.4.0",
]
lance = [
  "pylance",
]
robinhood = [
  "pynacl>=1.5.0,<2.0.0",
]
salesforce = [
  "simple-salesforce>=1.12.0,<2.0.0",
]
sftp = [
  "paramiko>=3.4.0,<4.0.0",
]
oracledb = [
  "oracledb>=2.0.0,<3.0.0",
]
all = [
  "faker>=23.1.0,<24.0.0",
  "datasets>=2.17.0,<3.0.0",
  "databricks-sdk>=0.28.0,<0.29.0",
  "kagglehub[pandas-datasets]>=0.3.10,<0.4.0",
  "pynacl>=1.5.0,<2.0.0",
  "simple-salesforce>=1.12.0,<2.0.0",
  "paramiko>=3.4.0,<4.0.0",
  "oracledb>=2.0.0,<3.0.0",
]

[tool.uv]
dev-dependencies = [
  "pytest>=8.0.0,<9.0.0",
  "pytest-cov>=4.0.0,<5.0.0",
  "grpcio>=1.60.1,<2.0.0",
  "grpcio-status>=1.60.1,<2.0.0",
  "pandas>=2.2.0,<3.0.0",
  "mkdocs-material>=9.5.40,<10.0.0",
  "pyspark==4.0.0",
  "ruff>=0.6.0,<0.7.0",
]

[tool.ruff]
line-length = 100
target-version = "py39"

[tool.ruff.format]
quote-style = "double"
indent-style = "space"

[tool.ruff.lint]
select = ["E", "F", "I", "UP"]
ignore = []

[tool.hatch.build.targets.wheel]
packages = ["pyspark_datasources"]

[tool.hatch.build.targets.sdist]
include = ["pyspark_datasources"]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"
