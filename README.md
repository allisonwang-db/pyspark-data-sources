# PySpark Data Sources

[![pypi](https://img.shields.io/pypi/v/pyspark-data-sources.svg?color=blue)](https://pypi.org/project/pyspark-data-sources/)
[![code style: ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)

Custom Apache Spark data sources using the [Python Data Source API](https://spark.apache.org/docs/4.0.0/api/python/tutorial/sql/python_data_source.html) (Spark 4.0+). Requires Apache Spark 4.0+ or [Databricks Runtime 15.4 LTS](https://docs.databricks.com/aws/en/release-notes/runtime/15.4lts)+.

## Quick Start

### Installation

```bash
pip install pyspark-data-sources
```

### Basic Usage

```python
from pyspark.sql import SparkSession
from pyspark_datasources import GithubDataSource

spark = SparkSession.builder.appName("datasource-demo").getOrCreate()
spark.dataSource.register(GithubDataSource)

# Read pull requests from a public GitHub repository (no API key required)
df = spark.read.format("github").load("apache/spark")
df.select("id", "title", "author").show()
# +---+--------------------+--------+
# | id|               title|  author|
# +---+--------------------+--------+
# |  1|Initial commit      |  matei |...
# +---+--------------------+--------+
```

## Available Data Sources

| Data Source | Read | Write | Description | Installation | Example |
|-------------|------|-------|-------------|--------------|--------|
| `arrow` | Batch | â€” | Read Apache Arrow files | built-in | [â†’](examples/arrow.md) |
| `fake` | Batch, Stream | â€” | Generate synthetic test data using Faker | built-in | [â†’](examples/fake.md) |
| `github` | Batch | â€” | Read GitHub pull requests | built-in | [â†’](examples/github.md) |
| `googlesheets` | Batch | â€” | Read public Google Sheets | built-in | [â†’](examples/googlesheets.md) |
| `huggingface` | Batch | â€” | Load Hugging Face datasets | `pip install pyspark-data-sources[datasets]` | [â†’](examples/huggingface.md) |
| `jira` | Batch | Batch | Read and write Jira issues | `pip install pyspark-data-sources[jira]` | [â†’](examples/jira.md) |
| `kaggle` | Batch | â€” | Load Kaggle datasets | `pip install pyspark-data-sources[kaggle]` | [â†’](examples/kaggle.md) |
| `lance` | â€” | Batch | Write Lance vector format | `pip install pyspark-data-sources[lance]` | [â†’](examples/lance.md) |
| `meta_capi` | â€” | Batch, Stream | Write to Meta Conversions API | built-in | [â†’](examples/meta_capi.md) |
| `notion` | Batch | Batch | Read and write Notion databases | `pip install pyspark-data-sources[notion]` | [â†’](examples/notion.md) |
| `opensky` | Batch, Stream | â€” | Live flight tracking data | built-in | [â†’](examples/opensky.md) |
| `robinhood` | Batch | â€” | Cryptocurrency market data from Robinhood API | `pip install pyspark-data-sources[robinhood]` | [â†’](examples/robinhood.md) |
| `salesforce` | â€” | Stream | Write to Salesforce objects | `pip install pyspark-data-sources[salesforce]` | [â†’](examples/salesforce.md) |
| `sftp` | Batch | Batch | Read/write files from SFTP server | `pip install pyspark-data-sources[sftp]` | [â†’](examples/sftp.md) |
| `oracle` | Batch | Batch | Read/write Oracle databases | `pip install pyspark-data-sources[oracledb]` | [â†’](examples/oracle.md) |
| `stock` | Batch | â€” | Fetch stock market data (Alpha Vantage) | built-in | [â†’](examples/stock.md) |
| `bored` | Batch | â€” | Activity suggestions from Bored API | built-in | [â†’](examples/bored.md) |
| `cocktail` | Batch | â€” | Cocktail recipes from TheCocktailDB | built-in | [â†’](examples/cocktail.md) |
| `dog` | Batch | â€” | Dog breeds from Dog CEO API | built-in | [â†’](examples/dog.md) |
| `httpbin` | Batch | â€” | Test data from httpbin.org | built-in | [â†’](examples/httpbin.md) |
| `ipapi` | Batch | â€” | IP geolocation from ip-api.com | built-in | [â†’](examples/ipapi.md) |
| `jsonlines` | Batch | â€” | Read .jsonl files | built-in | [â†’](examples/jsonlines.md) |
| `quotable` | Batch | â€” | Quotes from Quotable API | built-in | [â†’](examples/quotable.md) |
| `tomlfile` | Batch | â€” | Read TOML files | `pip install pyspark-data-sources[toml]` | [â†’](examples/tomlfile.md) |
| `universities` | Batch | â€” | University data from HipoLabs | built-in | [â†’](examples/universities.md) |
| `yamlfile` | Batch | â€” | Read YAML files | `pip install pyspark-data-sources[yaml]` | [â†’](examples/yamlfile.md) |
| `jsonplaceholder` | Batch | â€” | Read JSON data for testing | built-in | [â†’](examples/jsonplaceholder.md) |
| `weather` | Batch | â€” | Read current weather data (OpenWeatherMap) | built-in | [â†’](examples/weather.md) |

ðŸ“š **[See detailed examples â†’](docs/data-sources-guide.md)** Â· **[Copy-pastable examples â†’](examples/README.md)**

## Building Your Own Data Source

**Recommendation:** Leverage AI to speed up development. If you use [Cursor](https://cursor.com), try the `create-data-source` skill (in `.cursor/skills/`) to generate a full implementation with templates and a step-by-step checklist.

Here's a minimal example to get started:

```python
from pyspark.sql.datasource import DataSource, DataSourceReader
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

class MyCustomDataSource(DataSource):
    def name(self):
        return "mycustom"

    def schema(self):
        return StructType([
            StructField("id", IntegerType()),
            StructField("name", StringType())
        ])

    def reader(self, schema):
        return MyCustomReader(self.options, schema)

class MyCustomReader(DataSourceReader):
    def __init__(self, options, schema):
        self.options = options
        self.schema = schema

    def read(self, partition):
        # Your data reading logic here
        for i in range(10):
            yield (i, f"name_{i}")

# Register and use
spark.dataSource.register(MyCustomDataSource)
df = spark.read.format("mycustom").load()
```

ðŸ“– **[Complete guide with advanced patterns â†’](docs/building-data-sources.md)**

## Documentation

- ðŸ“š **[Data Sources Guide](docs/data-sources-guide.md)** - Common patterns and troubleshooting
- ðŸ”§ **[Building Data Sources](docs/building-data-sources.md)** - Complete tutorial with advanced patterns
- ðŸ“– **[API Reference](docs/api-reference.md)** - Full API specification and method signatures
- ðŸ’» **[Development Guide](contributing/DEVELOPMENT.md)** - Contributing and development setup

## Contributing

We welcome contributions! See our [Development Guide](contributing/DEVELOPMENT.md) for details.

## Resources

- [Python Data Source API Documentation](https://spark.apache.org/docs/4.0.0/api/python/tutorial/sql/python_data_source.html)
- [API Source Code](https://github.com/apache/spark/blob/master/python/pyspark/sql/datasource.py)

## Community Tools
- [Polymo](https://github.com/dan1elt0m/polymo) - Declarative REST API ingestion. [Medium article](https://medium.com/@d.e.tom89/turn-any-rest-api-into-spark-dataframes-in-minutes-with-polymo-028a48113eb1)